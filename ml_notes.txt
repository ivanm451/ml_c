
05/20/2023
Here is what we did for our first machine learning model

We have a data set like this:
0 0 
1 2
2 4
3 6
4 8

Then we build our model: y = x*w, so that it can predict any element of the second value.
In a real world usage, we wouldn't know what the result may be, but in this simple example we do so it helps.

w is our parameter. models can have many. 
x is our input to the model. the first column of values in our data set. 
d is the distance from our output to our actual value in second column of data set.
We use seeding (random number generation) to create random initial guess. 

By introducing cost function we can make our guesses better. 
If we think of our model as a function we can use derivatives to find the minima where d = 0 meaning we guessed correctly. 
But computer is discrete. Well, we can use finite difference approach to implement limit def of derivative. 

We program our cost function as its own function in the code and then we run it in for loop
The output of our cost function can then be used as function for finite difference which gently nudges us to a better guess.

We can have many inputs and parameters for our model.

Simple paramater model: y = x*w
More complex version: y = x1*w1 + x2*w2 + x3*w3 + ... + b

Where b is our bias. In general, having a bias in our model can help improve performance
Since this is another parameter, we can apply the finite difference method to it as well to optimize our function
LO AND BEHOLD, WE NOW HAVE GRADIENT DESCENT!

-- WE BUILD A MORE COMPLEX MODEL --
We can build a more complex neuron that can model OR, AND, XOR logic gate behavior

We implement main.c into gates.c, adding another dataset x2 and updating our code as necessary

To aid the performance of the model, we can implement an activation function, the simplest being a sigmoid function 

The problem of how to minimize cost function and optimize model is where lots of juicy maths and research revolves around

While testing the performance of our OR gates model we saw that it was performing poorly without a bias.

By adding a bias to our cost function we saw increased performance, by almost two magnitudes.

It still requires a lot of training, but for the same amount of training cycles adding bias drastically improved OR gate model 

